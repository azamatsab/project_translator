{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Проверка сохранения модели\n",
    "\n",
    "Построим тестовый ноутук для выбора оптимального пути сохранения и загрузки модели.\n",
    "\n",
    "При использовании `pytorch` пока найдено два способа:\n",
    "- `save_pretrained`\n",
    "- `save_dict` - он сложнее, но есть шанс, что будет меньше места занимать"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of MarianMTModel were not initialized from the model checkpoint at ../../opus-mt-en-ru and are newly initialized: ['lm_head.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, MarianMTModel\n",
    "\n",
    "DEFAULT_FILEPATH = \"../../opus-mt-en-ru\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_FILEPATH)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(DEFAULT_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(4.8305, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(\"My name is Wolfgang and I live in Berlin\", return_tensors='pt').input_ids\n",
    "\n",
    "decoder_input_ids = tokenizer(\"Меня зовут Вольфганг и я живу в Берлине.\", return_tensors='pt').input_ids\n",
    "\n",
    "output = model(input_ids=input_ids, labels=decoder_input_ids)\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_path = \"./store-pretrained\"\n",
    "tokenizer.save_pretrained(store_path)\n",
    "model.save_pretrained(store_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "root_directory = Path('.')\n",
    "sum(f.stat().st_size for f in root_directory.glob('**/*') if f.is_file())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "root_directory = Path(store_path)\n",
    "size_save_pretrained = sum(f.stat().st_size for f in root_directory.glob('**/*') if f.is_file())\n",
    "\n",
    "size_save_pretrained /= 2 ** 20\n",
    "\n",
    "root_directory = Path(DEFAULT_FILEPATH)\n",
    "size_original = sum(f.stat().st_size for f in root_directory.glob('**/*') if f.is_file())\n",
    "\n",
    "size_original /= 2 ** 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Storage memory of original model from HFace 1665.45 MB and stroed model by save_pretrained 295.2 MB.\n"
     ]
    }
   ],
   "source": [
    "print(\"Storage memory of original model from HFace {:g} MB and stroed model by save_pretrained {:g} MB.\".format(size_original, size_save_pretrained))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_tokenizer = AutoTokenizer.from_pretrained(store_path)\n",
    "loaded_model = AutoModelForSeq2SeqLM.from_pretrained(store_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(4.8305, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "input_ids = loaded_tokenizer(\"My name is Wolfgang and I live in Berlin\", return_tensors='pt').input_ids\n",
    "\n",
    "decoder_input_ids = loaded_tokenizer(\"Меня зовут Вольфганг и я живу в Берлине.\", return_tensors='pt').input_ids\n",
    "\n",
    "output = loaded_model(input_ids=input_ids, labels=decoder_input_ids)\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}